{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c340ffca-237a-414f-a5d1-6d26853a4848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T10:42:10.145027Z",
     "iopub.status.busy": "2025-05-31T10:42:10.144640Z",
     "iopub.status.idle": "2025-05-31T12:51:38.886147Z",
     "shell.execute_reply": "2025-05-31T12:51:38.884862Z"
    },
    "papermill": {
     "duration": 7768.751498,
     "end_time": "2025-05-31T12:51:38.888943",
     "exception": false,
     "start_time": "2025-05-31T10:42:10.137445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/gnn_project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 7856,Val: 3133,Test: 3333,Val + Train combined: 10989\n",
      "\n",
      "Data loading completed\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter optimization...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 10:42:20,151] A new study created in memory with name: no-name-5e680f19-054b-4259-b682-ea6b7cee6504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 10:47:17,384] Trial 0 finished with value: 0.6282645785808563 and parameters: {'hidden_dim': 107, 'dropout_rate': 0.2050714306409916, 'aggregator_type': 'pool', 'lr': 0.0029106359131330704, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 0 with value: 0.6282645785808563.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 10:52:14,302] Trial 1 finished with value: 0.640078740119934 and parameters: {'hidden_dim': 122, 'dropout_rate': 0.18080725777960455, 'aggregator_type': 'pool', 'lr': 0.00010994335574766199, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 0 with value: 0.6282645785808563.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 10:54:38,874] Trial 2 finished with value: 0.6268081665039062 and parameters: {'hidden_dim': 103, 'dropout_rate': 0.16247564316322377, 'aggregator_type': 'pool', 'lr': 0.0007309539835912913, 'batch_size': 256, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 2 with value: 0.6268081665039062.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 10:57:02,582] Trial 3 finished with value: 0.6264245235002958 and parameters: {'hidden_dim': 113, 'dropout_rate': 0.18851759613930136, 'aggregator_type': 'pool', 'lr': 0.00025081156860452336, 'batch_size': 256, 'n_hidden_layers': 1, 'lr_scheduler': 'StepLR', 'activation': 'RELU', 'step_size': 15, 'gamma_step': 0.9228439752154667}. Best is trial 3 with value: 0.6264245235002958.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 10:59:35,504] Trial 4 finished with value: 0.6222706253711994 and parameters: {'hidden_dim': 145, 'dropout_rate': 0.19083973481164612, 'aggregator_type': 'pool', 'lr': 0.0004066563313514797, 'batch_size': 256, 'n_hidden_layers': 2, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 4 with value: 0.6222706253711994.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 10:59:38,549] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:04:35,066] Trial 6 finished with value: 0.6291889607906341 and parameters: {'hidden_dim': 144, 'dropout_rate': 0.19948273504276487, 'aggregator_type': 'pool', 'lr': 0.0015696396388661157, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 4 with value: 0.6222706253711994.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:09:39,581] Trial 7 finished with value: 0.6478572618961335 and parameters: {'hidden_dim': 108, 'dropout_rate': 0.13713490317738958, 'aggregator_type': 'pool', 'lr': 0.004544383960336014, 'batch_size': 128, 'n_hidden_layers': 2, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 4 with value: 0.6222706253711994.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:09:43,355] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:09:57,810] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:09:59,724] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:10:01,536] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:10:03,431] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:10:05,007] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:10:06,684] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:10:08,557] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:10:10,087] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:10:17,190] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:10:18,771] Trial 18 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:12:51,806] Trial 19 finished with value: 0.6139960289001465 and parameters: {'hidden_dim': 97, 'dropout_rate': 0.19250430639813132, 'aggregator_type': 'pool', 'lr': 0.0014097199792258216, 'batch_size': 256, 'n_hidden_layers': 2, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 19 with value: 0.6139960289001465.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:15:24,217] Trial 20 finished with value: 0.6416184076896081 and parameters: {'hidden_dim': 95, 'dropout_rate': 0.11109046195900603, 'aggregator_type': 'pool', 'lr': 0.0016155377070117766, 'batch_size': 256, 'n_hidden_layers': 2, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 19 with value: 0.6139960289001465.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:15:26,062] Trial 21 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:17:59,254] Trial 22 finished with value: 0.6110562819700974 and parameters: {'hidden_dim': 97, 'dropout_rate': 0.17326303927623182, 'aggregator_type': 'pool', 'lr': 0.001282189997325581, 'batch_size': 256, 'n_hidden_layers': 2, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 22 with value: 0.6110562819700974.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:02,485] Trial 23 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:04,157] Trial 24 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:05,794] Trial 25 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:07,340] Trial 26 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:08,920] Trial 27 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:20,086] Trial 28 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:23,658] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:26,560] Trial 30 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:28,050] Trial 31 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:29,488] Trial 32 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:30,756] Trial 33 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:33,789] Trial 34 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:35,218] Trial 35 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:38,253] Trial 36 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:40,016] Trial 37 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:43,403] Trial 38 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:44,928] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:46,631] Trial 40 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:48,154] Trial 41 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:49,533] Trial 42 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:52,405] Trial 43 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:53,927] Trial 44 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:18:57,103] Trial 45 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:21:24,191] Trial 46 finished with value: 0.6182979207772475 and parameters: {'hidden_dim': 113, 'dropout_rate': 0.15677939106213717, 'aggregator_type': 'pool', 'lr': 0.0011809406490014614, 'batch_size': 256, 'n_hidden_layers': 2, 'lr_scheduler': 'StepLR', 'activation': 'RELU', 'step_size': 19, 'gamma_step': 0.9108293880901943}. Best is trial 22 with value: 0.6110562819700974.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:23:52,378] Trial 47 finished with value: 0.6566173067459693 and parameters: {'hidden_dim': 124, 'dropout_rate': 0.14315671149298714, 'aggregator_type': 'pool', 'lr': 0.0013049824004313798, 'batch_size': 256, 'n_hidden_layers': 2, 'lr_scheduler': 'StepLR', 'activation': 'RELU', 'step_size': 19, 'gamma_step': 0.9099970578322318}. Best is trial 22 with value: 0.6110562819700974.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:23:54,075] Trial 48 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:23:55,734] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:23:57,403] Trial 50 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:26:25,847] Trial 51 finished with value: 0.6170850579555218 and parameters: {'hidden_dim': 109, 'dropout_rate': 0.166172913234014, 'aggregator_type': 'pool', 'lr': 0.000775363233864636, 'batch_size': 256, 'n_hidden_layers': 2, 'lr_scheduler': 'StepLR', 'activation': 'RELU', 'step_size': 19, 'gamma_step': 0.8832285173472115}. Best is trial 22 with value: 0.6110562819700974.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:26:35,819] Trial 52 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:26:37,466] Trial 53 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:26:39,454] Trial 54 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:26:41,147] Trial 55 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:26:42,861] Trial 56 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:26:44,609] Trial 57 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:26:46,719] Trial 58 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:26:49,105] Trial 59 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:27:01,444] Trial 60 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:27:08,191] Trial 61 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:27:09,601] Trial 62 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:27:11,285] Trial 63 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:27:12,999] Trial 64 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:27:18,777] Trial 65 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:27:22,020] Trial 66 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:27:23,831] Trial 67 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:27:25,201] Trial 68 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:27:26,876] Trial 69 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:27:30,057] Trial 70 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:32:25,368] Trial 71 finished with value: 0.6383657288551331 and parameters: {'hidden_dim': 108, 'dropout_rate': 0.19965796654490658, 'aggregator_type': 'pool', 'lr': 0.006280580203014243, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 22 with value: 0.6110562819700974.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:32:37,401] Trial 72 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:32:43,379] Trial 73 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:37:39,007] Trial 74 finished with value: 0.6375794315338135 and parameters: {'hidden_dim': 109, 'dropout_rate': 0.1953632525991438, 'aggregator_type': 'pool', 'lr': 0.004149023300197253, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 22 with value: 0.6110562819700974.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:37:40,459] Trial 75 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:37:52,466] Trial 76 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:37:53,994] Trial 77 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:37:59,721] Trial 78 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:38:01,266] Trial 79 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:38:03,126] Trial 80 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:38:15,181] Trial 81 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:43:09,057] Trial 82 finished with value: 0.6321249616146087 and parameters: {'hidden_dim': 143, 'dropout_rate': 0.1913530803584482, 'aggregator_type': 'pool', 'lr': 0.002245460558091517, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 22 with value: 0.6110562819700974.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:43:21,209] Trial 83 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:43:24,306] Trial 84 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:43:36,931] Trial 85 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:43:38,386] Trial 86 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:43:40,051] Trial 87 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:43:41,438] Trial 88 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:43:44,642] Trial 89 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:43:46,241] Trial 90 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:44:58,259] Trial 91 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:45:18,929] Trial 92 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:45:36,618] Trial 93 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:45:54,405] Trial 94 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:46:06,288] Trial 95 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:46:09,245] Trial 96 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:46:11,084] Trial 97 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:46:12,741] Trial 98 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:46:18,985] Trial 99 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:46:20,657] Trial 100 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:46:29,665] Trial 101 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:46:41,377] Trial 102 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:51:36,794] Trial 103 finished with value: 0.6430990040302277 and parameters: {'hidden_dim': 107, 'dropout_rate': 0.19972885528155715, 'aggregator_type': 'pool', 'lr': 0.005067540018024504, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 22 with value: 0.6110562819700974.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:56:30,831] Trial 104 finished with value: 0.6413699495792389 and parameters: {'hidden_dim': 116, 'dropout_rate': 0.19144423987244757, 'aggregator_type': 'pool', 'lr': 0.0038801386803929694, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 22 with value: 0.6110562819700974.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:56:37,157] Trial 105 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:56:38,559] Trial 106 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:56:41,601] Trial 107 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:56:42,987] Trial 108 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:56:44,385] Trial 109 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 11:58:00,303] Trial 110 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:02:55,149] Trial 111 finished with value: 0.6297106957435608 and parameters: {'hidden_dim': 107, 'dropout_rate': 0.20026335486892047, 'aggregator_type': 'pool', 'lr': 0.007208320935746007, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 22 with value: 0.6110562819700974.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:03:16,265] Trial 112 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:03:19,313] Trial 113 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:03:22,355] Trial 114 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:08:17,852] Trial 115 finished with value: 0.6100052690505982 and parameters: {'hidden_dim': 103, 'dropout_rate': 0.1945178681889085, 'aggregator_type': 'pool', 'lr': 0.002066595185419995, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 115 with value: 0.6100052690505982.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:08:19,326] Trial 116 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:08:21,094] Trial 117 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:08:30,349] Trial 118 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:08:31,935] Trial 119 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:08:35,104] Trial 120 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:08:38,342] Trial 121 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:08:41,450] Trial 122 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:08:44,531] Trial 123 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:13:41,207] Trial 124 finished with value: 0.6219936108589172 and parameters: {'hidden_dim': 103, 'dropout_rate': 0.1568245922967198, 'aggregator_type': 'pool', 'lr': 0.0022444119462939754, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 115 with value: 0.6100052690505982.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:13:44,316] Trial 125 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:13:45,862] Trial 126 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:13:47,453] Trial 127 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:18:43,464] Trial 128 finished with value: 0.6218214321136475 and parameters: {'hidden_dim': 97, 'dropout_rate': 0.18874855660072504, 'aggregator_type': 'pool', 'lr': 0.0024919033938664336, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 115 with value: 0.6100052690505982.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:18:44,985] Trial 129 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:18:48,135] Trial 130 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:18:54,115] Trial 131 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:18:57,089] Trial 132 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:19:00,117] Trial 133 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:19:12,259] Trial 134 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:19:15,755] Trial 135 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:19:17,427] Trial 136 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:19:20,560] Trial 137 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:19:22,135] Trial 138 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:19:33,941] Trial 139 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:19:35,492] Trial 140 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:19:41,592] Trial 141 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:19:44,660] Trial 142 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:24:39,462] Trial 143 finished with value: 0.6342174172401428 and parameters: {'hidden_dim': 100, 'dropout_rate': 0.19346903179665811, 'aggregator_type': 'pool', 'lr': 0.00800267925875734, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 115 with value: 0.6100052690505982.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:29:35,494] Trial 144 finished with value: 0.630817768573761 and parameters: {'hidden_dim': 100, 'dropout_rate': 0.19268432752916687, 'aggregator_type': 'pool', 'lr': 0.008555084077538426, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 115 with value: 0.6100052690505982.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:29:38,707] Trial 145 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:29:41,668] Trial 146 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:29:50,099] Trial 147 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:29:53,214] Trial 148 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:29:54,707] Trial 149 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:29:57,941] Trial 150 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:30:00,885] Trial 151 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:34:57,540] Trial 152 finished with value: 0.6392813718318939 and parameters: {'hidden_dim': 100, 'dropout_rate': 0.18967920191814383, 'aggregator_type': 'pool', 'lr': 0.006404046147694083, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}. Best is trial 115 with value: 0.6100052690505982.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:35:12,484] Trial 153 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:35:15,611] Trial 154 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:35:18,766] Trial 155 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:35:20,230] Trial 156 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:35:23,345] Trial 157 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:35:24,733] Trial 158 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 12:35:28,619] Trial 159 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters are {'hidden_dim': 103, 'dropout_rate': 0.1945178681889085, 'aggregator_type': 'pool', 'lr': 0.002066595185419995, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}.\n",
      "\n",
      "Trial #1\n",
      "  Value (Objective): 0.6100052690505982\n",
      "  Params: {'hidden_dim': 103, 'dropout_rate': 0.1945178681889085, 'aggregator_type': 'pool', 'lr': 0.002066595185419995, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}\n",
      "\n",
      "Trial #2\n",
      "  Value (Objective): 0.6110562819700974\n",
      "  Params: {'hidden_dim': 97, 'dropout_rate': 0.17326303927623182, 'aggregator_type': 'pool', 'lr': 0.001282189997325581, 'batch_size': 256, 'n_hidden_layers': 2, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}\n",
      "\n",
      "Trial #3\n",
      "  Value (Objective): 0.6139960289001465\n",
      "  Params: {'hidden_dim': 97, 'dropout_rate': 0.19250430639813132, 'aggregator_type': 'pool', 'lr': 0.0014097199792258216, 'batch_size': 256, 'n_hidden_layers': 2, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}\n",
      "\n",
      "Trial #4\n",
      "  Value (Objective): 0.6170850579555218\n",
      "  Params: {'hidden_dim': 109, 'dropout_rate': 0.166172913234014, 'aggregator_type': 'pool', 'lr': 0.000775363233864636, 'batch_size': 256, 'n_hidden_layers': 2, 'lr_scheduler': 'StepLR', 'activation': 'RELU', 'step_size': 19, 'gamma_step': 0.8832285173472115}\n",
      "\n",
      "Trial #5\n",
      "  Value (Objective): 0.6182979207772475\n",
      "  Params: {'hidden_dim': 113, 'dropout_rate': 0.15677939106213717, 'aggregator_type': 'pool', 'lr': 0.0011809406490014614, 'batch_size': 256, 'n_hidden_layers': 2, 'lr_scheduler': 'StepLR', 'activation': 'RELU', 'step_size': 19, 'gamma_step': 0.9108293880901943}\n",
      "\n",
      "Trial #6\n",
      "  Value (Objective): 0.6192989122867584\n",
      "  Params: {'hidden_dim': 143, 'dropout_rate': 0.16081167654540515, 'aggregator_type': 'pool', 'lr': 0.0020933079019169396, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}\n",
      "\n",
      "Trial #7\n",
      "  Value (Objective): 0.6218214321136475\n",
      "  Params: {'hidden_dim': 97, 'dropout_rate': 0.18874855660072504, 'aggregator_type': 'pool', 'lr': 0.0024919033938664336, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}\n",
      "\n",
      "Trial #8\n",
      "  Value (Objective): 0.6219936108589172\n",
      "  Params: {'hidden_dim': 103, 'dropout_rate': 0.1568245922967198, 'aggregator_type': 'pool', 'lr': 0.0022444119462939754, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}\n",
      "\n",
      "Trial #9\n",
      "  Value (Objective): 0.6222706253711994\n",
      "  Params: {'hidden_dim': 145, 'dropout_rate': 0.19083973481164612, 'aggregator_type': 'pool', 'lr': 0.0004066563313514797, 'batch_size': 256, 'n_hidden_layers': 2, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}\n",
      "\n",
      "Trial #10\n",
      "  Value (Objective): 0.6264245235002958\n",
      "  Params: {'hidden_dim': 113, 'dropout_rate': 0.18851759613930136, 'aggregator_type': 'pool', 'lr': 0.00025081156860452336, 'batch_size': 256, 'n_hidden_layers': 1, 'lr_scheduler': 'StepLR', 'activation': 'RELU', 'step_size': 15, 'gamma_step': 0.9228439752154667}\n",
      "Hyperparameter optimization done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1065417/1537546292.py:201: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()       # initializing the gradient scaler\n",
      "/tmp/ipykernel_1065417/1537546292.py:215: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():    # automatically selects the appropriate floating-point precision (to optimize performance - speeds up training, reduces memory usage)\n",
      "/home/ubuntu/miniconda3/envs/gnn_project/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:157: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.\n",
      "  return th.cuda.amp.autocast_mode._cast(\n",
      "/home/ubuntu/miniconda3/envs/gnn_project/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:148: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return th.cuda.amp.autocast(enabled=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'hidden_dim': 103, 'dropout_rate': 0.1945178681889085, 'aggregator_type': 'pool', 'lr': 0.002066595185419995, 'batch_size': 128, 'n_hidden_layers': 1, 'lr_scheduler': 'ExponentialLR', 'activation': 'RELU', 'gamma_exp': 0.9872745703157462}\n",
      "Dataloaders done.\n",
      "Retraining using best parameters...\n",
      "Number of available node features (in_feats): 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1065417/1537546292.py:247: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():    # automatically selects the appropriate floating-point precision (to optimize performance - speeds up training, reduces memory usage)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000Train loss: 0.6880Val loss: 0.6807Val accuracy: 59.94% MCC: 0.10575929824221235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000Train loss: 0.6307Val loss: 0.6262Val accuracy: 67.57% MCC: 0.3304443736308713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000Train loss: 0.6161Val loss: 0.6212Val accuracy: 67.70% MCC: 0.3230015867762452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000Train loss: 0.6079Val loss: 0.6409Val accuracy: 67.19% MCC: 0.3027153221895096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000Train loss: 0.5998Val loss: 0.6304Val accuracy: 67.89% MCC: 0.3201112904168593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000Train loss: 0.6000Val loss: 0.6121Val accuracy: 68.46% MCC: 0.3543886181304102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000Train loss: 0.5966Val loss: 0.6245Val accuracy: 68.37% MCC: 0.33528078813996387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000Train loss: 0.5929Val loss: 0.6162Val accuracy: 68.78% MCC: 0.35343290575640235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000Train loss: 0.5859Val loss: 0.6269Val accuracy: 68.43% MCC: 0.33699638437814805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000Train loss: 0.5855Val loss: 0.6178Val accuracy: 69.10% MCC: 0.36004875512233303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000Train loss: 0.5841Val loss: 0.6148Val accuracy: 68.21% MCC: 0.348830295357906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000Train loss: 0.5819Val loss: 0.6211Val accuracy: 69.10% MCC: 0.3601801793180755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000Train loss: 0.5784Val loss: 0.6215Val accuracy: 69.14% MCC: 0.3580265754610364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000Train loss: 0.5829Val loss: 0.6188Val accuracy: 68.94% MCC: 0.35857422036229814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/1000Train loss: 0.5773Val loss: 0.6259Val accuracy: 68.72% MCC: 0.34351710849430533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/1000Train loss: 0.5702Val loss: 0.6211Val accuracy: 69.10% MCC: 0.3599178982037118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/1000Train loss: 0.5720Val loss: 0.6217Val accuracy: 68.91% MCC: 0.3544079845223455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/1000Train loss: 0.5723Val loss: 0.6215Val accuracy: 69.10% MCC: 0.35940014898039757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/1000Train loss: 0.5715Val loss: 0.6198Val accuracy: 69.17% MCC: 0.36124142396836495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/1000Train loss: 0.5722Val loss: 0.6283Val accuracy: 68.94% MCC: 0.3494105504233686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000Train loss: 0.5690Val loss: 0.6288Val accuracy: 68.56% MCC: 0.33735179509312774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/1000Train loss: 0.5694Val loss: 0.6276Val accuracy: 68.62% MCC: 0.34134478988032757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/1000Train loss: 0.5739Val loss: 0.6266Val accuracy: 68.75% MCC: 0.3469739821855552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/1000Train loss: 0.5682Val loss: 0.6260Val accuracy: 68.78% MCC: 0.3461764794252838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/1000Train loss: 0.5652Val loss: 0.6257Val accuracy: 69.10% MCC: 0.3568318203603828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/1000Train loss: 0.5656Val loss: 0.6288Val accuracy: 68.43% MCC: 0.33849882988034574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/1000Train loss: 0.5652Val loss: 0.6270Val accuracy: 68.75% MCC: 0.345647749181823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/1000Train loss: 0.5654Val loss: 0.6247Val accuracy: 69.04% MCC: 0.3559699244540303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/1000Train loss: 0.5641Val loss: 0.6220Val accuracy: 69.29% MCC: 0.3632481630660553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/1000Train loss: 0.5666Val loss: 0.6222Val accuracy: 68.94% MCC: 0.3555119249814679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/1000Train loss: 0.5642Val loss: 0.6249Val accuracy: 69.10% MCC: 0.3575398254080821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/1000Train loss: 0.5661Val loss: 0.6229Val accuracy: 68.98% MCC: 0.35727084667440556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/1000Train loss: 0.5604Val loss: 0.6252Val accuracy: 68.98% MCC: 0.35675019942176783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/1000Train loss: 0.5594Val loss: 0.6271Val accuracy: 69.10% MCC: 0.3567158383568016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/1000Train loss: 0.5657Val loss: 0.6310Val accuracy: 68.43% MCC: 0.3369074645491624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/1000Train loss: 0.5661Val loss: 0.6267Val accuracy: 68.75% MCC: 0.34932160746519986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/1000Train loss: 0.5609Val loss: 0.6306Val accuracy: 68.43% MCC: 0.33791793709343865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/1000Train loss: 0.5613Val loss: 0.6218Val accuracy: 68.98% MCC: 0.3610199516822703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/1000Train loss: 0.5624Val loss: 0.6255Val accuracy: 68.37% MCC: 0.3430559832408127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/1000Train loss: 0.5618Val loss: 0.6295Val accuracy: 68.46% MCC: 0.3403430650417293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 198\n",
      "The best epoch was 118\n",
      "Training done.\n",
      "Final training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done.\n",
      "Evaluating on test_dataset\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5783030986785889\n",
      "Accuracy: 0.7044704470447045, Precision: 0.7562588568729334, Recall: 0.7734299516908213, F1-Score: 0.7647480296154765, MCC: 0.3677124725062576, ROC-AUC: 0.7648922701489055\n",
      "Evaluation done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import torch\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "import random\n",
    "\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import SAGEConv, GlobalAttentionPooling\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "# subclass of torch.nn.Module\n",
    "\n",
    "# subclass of torch.nn.Module\n",
    "\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_feats,       # input feature dimensions (anotation size)\n",
    "        out_feats,    # the same thing as hidden_dim - number of dimentions of hi+1\n",
    "        aggregator_type,    # type of aggregation\n",
    "        dropout_rate=0.5, # the dropout rate \n",
    "        n_cls=0.2,\n",
    "        n_hidden_layers=2,\n",
    "        activation='RELU'\n",
    "        ):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.dropout = nn.Dropout(dropout_rate) # dropout layer\n",
    "        self.in_feats = in_feats\n",
    "        self.out_feats = out_feats\n",
    "        self.n_cls = n_cls\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        for i in range(n_hidden_layers):\n",
    "            if i == 0:\n",
    "                set_in_feats = self.in_feats\n",
    "            else:\n",
    "                set_in_feats = self.out_feats\n",
    "            layer = SAGEConv(\n",
    "                in_feats=set_in_feats,\n",
    "                out_feats=out_feats,\n",
    "                aggregator_type=aggregator_type,\n",
    "                feat_drop=dropout_rate,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "        self.pooling = GlobalAttentionPooling(nn.Linear(self.out_feats, 1))\n",
    "            # --> aggregating the features into a single graph-level representation\n",
    "            # gate mechanism determines the importance of each node\n",
    "        self.output_layer = nn.Linear(self.out_feats, n_cls)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, graph, labels):\n",
    "        #feat = self.dropout(labels)\n",
    "        if (self.activation == 'RELU'):\n",
    "            h = F.relu(self.layers[0](graph, labels))\n",
    "        elif (self.activation == 'LeakyRELU'):\n",
    "            h = F.leaky_relu(self.layers[0](graph, labels), negative_slope=0.01)\n",
    "        #h = F.relu(self.layers[0](graph, labels))\n",
    "        h = self.dropout(h)\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            # print(f\"SHAPE OF h IS: {graph.ndata['h'].size()}\")\n",
    "            # print(f\"NUM NODES: {graph.num_nodes()} \")\n",
    "            if i == 0:\n",
    "                continue\n",
    "            else:\n",
    "                if (self.activation == 'RELU'):\n",
    "                    h = F.relu(self.layers[i](graph, h))\n",
    "                elif (self.activation == 'LeakyRELU'):\n",
    "                    h = F.leaky_relu(self.layers[i](graph, h), negative_slope=0.01)\n",
    "                if i < self.n_hidden_layers - 1:\n",
    "                    h = self.dropout(h)\n",
    "        h = self.pooling(graph, h)#.squeeze\n",
    "        h = self.output_layer(h)\n",
    "        return h\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "            \n",
    "\n",
    "class EarlyStopping():\n",
    "    # implementing early stopping mechanism, but there is no printing method implemented\n",
    "    \"\"\"Early stops the training if neither validation loss nor validation\n",
    "    accuracy improves after their respective patience levels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    patience_loss : int\n",
    "        How long to wait after last time validation loss improved.\n",
    "    patience_accuracy : int\n",
    "        How long to wait after last time validation accuracy improved.\n",
    "    verbose : bool\n",
    "        If True, prints a message for each validation metric improvement.\n",
    "    delta_loss : float\n",
    "        Minimum change in the validation loss to qualify as an improvement.\n",
    "    delta_accuracy : float\n",
    "        Minimum change in the validation accuracy to qualify as an improvement.\n",
    "    path : str\n",
    "        The file path where the model will be saved.\n",
    "    print_freq : int\n",
    "        The frequency at which to print messages during training.\n",
    "\n",
    "        - taken from Jozef's master's thesis\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        patience_loss=10,\n",
    "        patience_mcc=10,\n",
    "        verbose=True,\n",
    "        delta_loss=0.001,\n",
    "        delta_mcc=0.001,\n",
    "    ):\n",
    "        self.patience_loss = patience_loss\n",
    "        self.patience_mcc = patience_mcc\n",
    "        self.verbose = verbose\n",
    "        self.loss_counter = 0\n",
    "        self.mcc_counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.best_mcc = -1             # stores best accuracy so far\n",
    "        self.early_stop = False             # bool indicating whether training should stop\n",
    "        self.delta_mcc = 0.001\n",
    "        self.delta_loss = 0.001\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    # in this method, we are monitoring both the validation loss and accuracy\n",
    "    def __call__(self, val_loss, val_acc, val_mcc, model, epoch):   # earlier the method was called '__step__'\n",
    "        improved_loss = False\n",
    "        improved_mcc = False\n",
    "\n",
    "        if val_loss < self.best_loss - self.delta_loss:     # True only if val_loss improves beyond best_loss - delta_loss (treshold)\n",
    "            self.best_loss = val_loss                           # delta_loss is a small treshold, which ought to prevent fluctuations\n",
    "            self.loss_counter = 0                               # in the documentation implementation, there is a strict comparison without taking into account the fluctuations\n",
    "            improved_loss = True\n",
    "        else:\n",
    "            self.loss_counter += 1\n",
    "\n",
    "        if val_mcc > self.best_mcc + self.delta_mcc: # True if val_accuracy improves bexond best_accuracy + delta_accuracy (treshold)\n",
    "            self.best_mcc = val_mcc\n",
    "            self.mcc_counter = 0\n",
    "            improved_mcc = True\n",
    "        else:\n",
    "            self.mcc_counter += 1\n",
    "\n",
    "        if improved_loss or improved_mcc:              # if one of the metrics improves, we save the state as a checkpoint\n",
    "            self.save_checkpoint(model, val_loss, val_mcc, val_acc)\n",
    "            self.best_epoch = epoch\n",
    "\n",
    "        if self.loss_counter >= self.patience_loss and self.mcc_counter >= self.patience_mcc: #\n",
    "            self.early_stop = True\n",
    "            if self.verbose:\n",
    "                print(\"Early stopping triggered\")\n",
    "\n",
    "    def save_checkpoint(self, model, val_loss, val_mcc, val_acc):   # we save the model when either the accuracy or loss improves\n",
    "        torch.save(model.state_dict(), \"sage_checkpoint.pt\")\n",
    "        if self.verbose:\n",
    "            print(f\"Checkpoint saved, mcc: {val_mcc}, loss: {val_loss}, accuracy: {val_acc}\")\n",
    "\n",
    "class Training:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def train_and_evaluate(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        early_stopping,\n",
    "        num_epochs,\n",
    "        # plot_curves=False,   for plotting curves\n",
    "        accumulation_steps=2,\n",
    "        scheduler=\"None\"\n",
    "    ):\n",
    "        train_losses, val_losses = [], []\n",
    "        scaler = GradScaler()       # initializing the gradient scaler\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            optimizer.zero_grad     # we initialize the gradients to zero, so that the gradients from the previous batch do not accumulate\n",
    "\n",
    "            for batch_idx, (batched_graph, labels) in enumerate(train_loader):  # train_loader ... data loader probiding batches of data\n",
    "                batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)   # self.device ... typically CPU or GPU\n",
    "                                                                                                # - by moving the data over there we ensure that both the data and the model are on the same device\n",
    "                                                                                                # that ensures no errors and consistency\n",
    "                batched_graph.ndata['h'] = batched_graph.ndata['h'].float().to(self.device) # Move node features to device - by bot TODO TODO TODO\n",
    "\n",
    "\n",
    "\n",
    "                with autocast():    # automatically selects the appropriate floating-point precision (to optimize performance - speeds up training, reduces memory usage)\n",
    "                    logits = model(batched_graph, batched_graph.ndata['h'].float()) # rund the model inside the autocast() context\n",
    "                    loss = criterion(logits, labels) / accumulation_steps   # we are using the gradient accumulation, because the batch size is too big to fit in memory\n",
    "                    # therefore we accumulate gradients over multiple batches before we update the weight --> loss is scaled down by the number of accumulation steps\n",
    "                scaler.scale(loss).backward()   # scaling the loss, preventing very small gradients from becoming zero (common issue in mixed precission training (autocast)\n",
    "                                                # .backward - computing the gradient using backpropagation\n",
    "                train_loss += loss.item() * accumulation_steps  # .item() ... converts loss from tensor to a python float\n",
    "                # we multiply by accumulation_steps to scale it back (we scaled down/divided earlier)\n",
    "                if (batch_idx + 1) % accumulation_steps == 0 or batch_idx == len(train_loader) - 1: # accumulating gradients for multiple batches before updatiing the model\n",
    "                # update does not happen after every batch\n",
    "                    scaler.step(optimizer)  # applying the scaled gradients to update model parameters\n",
    "                    scaler.update()         # ... updates the scaling factor for the next iteration\n",
    "                    # dynamically adjusts the scaling value to mantain stable gradients - if gradients too small, increases the scale and vice versa\n",
    "                    optimizer.zero_grad()   # initializing gradients to zero - clearing the gradients before the next batch\n",
    "\n",
    "            train_loss = train_loss/len(train_loader)   # getting the average loss per batch\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            validation_loss = 0.0\n",
    "            validation_accuraccy = 0.0\n",
    "            num_val_correct = 0         # number of correctly predicted samples\n",
    "            num_total = 0               # total number of samples\n",
    "            TP, TN, FP, FN = 0, 0, 0, 0\n",
    "            if val_loader is not None:  # val_loader ... validation dataset, we check, whether it is not null --> then there would be no validation\n",
    "                model.eval()            # we put the model into the evaluation mode - we turn off the dropout layers and disable the batch normalization updates\n",
    "                                        # --> validation results are consistent and unaffected by randomness\n",
    "                with torch.no_grad():   # we prevent pytorch from storing gradients during validation --> saves memory and improves performance\n",
    "                    for batched_graph, labels in val_loader:        # iterates over mini-batches of validation data\n",
    "                        batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)   # we move the input and labels to the correct device (CPU, GPU)\n",
    "\n",
    "                        batched_graph.ndata['h'] = batched_graph.ndata['h'].to(self.device) # TODO TODO TODO - chatova práce\n",
    "\n",
    "                        with autocast():    # automatically selects the appropriate floating-point precision (to optimize performance - speeds up training, reduces memory usage)\n",
    "                            logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                            # graph structure (batched_graph) is passed on to the model, node features (batched_graph.ndata['h']) serve as input data\n",
    "                                # --> The GNN layers aggregate information from neighboring nodes\n",
    "                                # final layer outputs predictions (logits) for node/graph classification\n",
    "                            loss = criterion(logits, labels)    # we compute the loss\n",
    "                        validation_loss += loss             # accumulating loss over all batches\n",
    "                        _, predicted = torch.max(logits.data, 1)    # getting the predicted class (highest probability)\n",
    "                        num_total += labels.size(0)             # updating the total number of samples\n",
    "                        num_val_correct += (predicted == labels).sum().item()   # adds up the number of correct predictions\n",
    "                        TP += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "                        TN += ((predicted == 0) & (labels == 0)).sum().item()\n",
    "                        FP += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "                        FN += ((predicted == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "                    num = TP * TN - FP * FN\n",
    "                    den = math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "                    validation_mcc = num / den if den > 0 else 0\n",
    "                    validation_loss = validation_loss/len(val_loader)   # we get the average loss\n",
    "                    val_losses.append(validation_loss)\n",
    "                    validation_accuraccy = num_val_correct/num_total    # saving for early stopping\n",
    "                    if early_stopping:  # checking if early stopping is not None\n",
    "                        early_stopping(validation_loss, validation_accuraccy, validation_mcc, model, epoch + 1)\n",
    "                        if early_stopping.early_stop:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                            break\n",
    "                    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                        print(f'Epoch {epoch + 1}/{num_epochs}'\n",
    "                              f'Train loss: {train_loss:.4f}'\n",
    "                              f'Val loss: {validation_loss:.4f}'\n",
    "                              f'Val accuracy: {100 * validation_accuraccy:.2f}% '\n",
    "                              f'MCC: {validation_mcc}')\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(validation_loss)\n",
    "            elif scheduler is not None:  # using the scheduler\n",
    "                scheduler.step()\n",
    "                # plotting of curves might be implemented here (TODO)\n",
    "\n",
    "        # there the plot curves method might be implemented (another TODO possibility)\n",
    "\n",
    "    def evaluate_on_test(self, model, test_loader, criterion, run_id=1):\n",
    "        model.eval()    # we put model into the evaluation model (ensuring that gradients won't be computed)\n",
    "        test_loss = 0.0\n",
    "        all_preds = []        # storing all predicted labels\n",
    "        all_labels = []       # storing all labels (true labels)\n",
    "        all_proba = []\n",
    "        with torch.no_grad():       # disabling calculations of gradient\n",
    "            for batched_graph, labels in test_loader:       # iterates over mini-batches of validation data\n",
    "                batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)       # we move the input and labels to the correct device (CPU, GPU)\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                test_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy()) # moving predictions and labels to CPU and converting them to NumPy arrays\n",
    "                #all_labels.extend(preds.cpu().numpy()) # (they get stored in all_pred ad all_labels variables)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_proba.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy()) \n",
    "                #all_proba\n",
    "        test_loss = test_loss/len(test_loader)  # averages the total loss over all test batches\n",
    "        # calculating evaluation metrics:\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "        matthews_corr = matthews_corrcoef(all_labels, all_preds)\n",
    "         # Compute ROC curve ---------\n",
    "        fpr, tpr, _ = roc_curve(all_labels, all_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        # Plot ROC Curve --------\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random classifier\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "        plt.savefig(\"sage_plot.png\")\n",
    "\n",
    "        # confusion matrix\n",
    "        conf_mat = confusion_matrix(all_labels, all_preds)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
    "        disp.plot(cmap='Blues')\n",
    "        plt.savefig(\"conf_mat_sage.png\")\n",
    "        # possible to save here (TODO) ?\n",
    "\n",
    "        results_dataFrame = pd.DataFrame({\n",
    "            'Test Loss': [test_loss],\n",
    "            'Accuracy': [accuracy],\n",
    "            'Precision': [precision],\n",
    "            'Recall': [recall],\n",
    "            'F1-Score': [f1],\n",
    "            'MCC': [matthews_corr],\n",
    "            'ROC-AUC': [roc_auc]\n",
    "        })\n",
    "        results_dataFrame.to_csv('test_results_SAGE.csv')  # possible to adjust settings\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}, MCC: {matthews_corr}, ROC-AUC: {roc_auc}\")\n",
    "\n",
    "def collate(samples):   # converts individual graph samples into a single batch for training\n",
    "                        # input ... samples - a list of tuples, where each tuple contains (graph, label)\n",
    "                        # graph ... dgl graph object, label ... a target label\n",
    "    graphs, labels = map(list, zip(*samples))   # unpacking graphs and labels from the list of tuples and converting them into separate lists\n",
    "    batched_graph = dgl.batch(graphs)           # batching the graphs (sdružování grafů)\n",
    "    labels = torch.tensor(labels, dtype=torch.long) # converting list of labels into a PyTorch tensor\n",
    "    return batched_graph, labels\n",
    "                        # batched graph ... single batched graph combining individual graphs\n",
    "                        # labels ... tensor of labels for the batch\n",
    "\n",
    "class Hyperparameter_optimizer:     # optuna ... library for automatic hyperparameter tuning, selecting the best hyperparameters based on validation loss\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,                     # GPU/CPU\n",
    "        subset_train_graphs,        # training data (graphs + labels)\n",
    "        subset_train_labels,\n",
    "        subset_val_graphs,          # validation data (graphs + labels)\n",
    "        subset_val_labels,\n",
    "        num_trials,                 # number of trials for optimization\n",
    "        num_epochs,                  # number of training epochs per trial\n",
    "        random_state\n",
    "    ):\n",
    "        self.device = device        # setting all of the parameters of the class\n",
    "        self.subset_train_graphs = subset_train_graphs\n",
    "        self.subset_train_labels = subset_train_labels\n",
    "        self.subset_val_graphs = subset_val_graphs\n",
    "        self.subset_val_labels = subset_val_labels\n",
    "        self.num_trials = num_trials\n",
    "        self.num_epochs = num_epochs\n",
    "        self.random_state = random_state\n",
    "    def objective(self, trial):     # sample hyperparameters, optuna is doing a \"smart\" selection of hyperparameters, which are most likely to give the best result\n",
    "        # optuna does not try all the combinations, since that would simply take too long\n",
    "        in_feats = 74\n",
    "        #hidden_dim = trial.suggest_int('hidden_dim', 74, 256)\n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 84, 147)\n",
    "        \n",
    "        #dropout_rate = trial.suggest_float('dropout_rate', 0.05, 0.5)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.11, 0.21)\n",
    "        \n",
    "        #aggregator_type = trial.suggest_categorical('aggregator_type', ['mean', 'gcn', 'pool', 'lstm'])\n",
    "        aggregator_type = trial.suggest_categorical('aggregator_type', ['pool'])\n",
    "        \n",
    "        #lr = trial.suggest_float('lr', 1e-6, 1e-1, log=True)\n",
    "        lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "        \n",
    "        #batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])#, 512])\n",
    "        batch_size = trial.suggest_categorical('batch_size', [128, 256])\n",
    "        \n",
    "        #n_hidden_layers = trial.suggest_int('n_hidden_layers', 1, 6)\n",
    "        n_hidden_layers = trial.suggest_int('n_hidden_layers', 1, 3)\n",
    "        \n",
    "        #lr_scheduler = trial.suggest_categorical('lr_scheduler', ['None', 'StepLR', 'ExponentialLR'])\n",
    "        lr_scheduler = trial.suggest_categorical('lr_scheduler', ['StepLR', 'ExponentialLR'])\n",
    "        \n",
    "        #activation = trial.suggest_categorical('activation', ['RELU', 'LeakyRELU'])\n",
    "        activation = trial.suggest_categorical('activation', ['RELU'])\n",
    "\n",
    "        model = SAGE(              # initializing model with sampled hyperparameters\n",
    "            in_feats=74,\n",
    "            out_feats=hidden_dim,\n",
    "            dropout_rate=dropout_rate,\n",
    "            aggregator_type=aggregator_type,\n",
    "            n_cls=2,\n",
    "            n_hidden_layers=n_hidden_layers,\n",
    "            activation = activation             # set differently?? TODO TODO TODO\n",
    "        #)      TODO TODO TODO - this is where the model is moved to the device (written by bot)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        class_counts = np.bincount(self.subset_train_labels)\n",
    "        class_counts[class_counts == 0] = 1\n",
    "        class_weights = 1.0 / class_counts\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)   # using Adam optimizer\n",
    "        #criterion = nn.CrossEntropyLoss().to(self.device)   # criterion for the classification task\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights).to(self.device)   # criterion for the classification task\n",
    "        # creating Dataloaders for training and validation - batches graphs and labels for training & validation\n",
    "            # Dataloaders in dgl ... handles graphs instead of regular tensors, creates mini-batches for efficient training, allows parallel processing, ...\n",
    "                                    # schuffles training data to prevent bias\n",
    "        scheduler_obj = None\n",
    "        if lr_scheduler == 'None':\n",
    "            scheduler_obj = None\n",
    "        elif lr_scheduler == 'StepLR':\n",
    "            #step_size = trial.suggest_int('step_size', 1, 20)\n",
    "            #step_size = trial.suggest_int('step_size', 14, 20)\n",
    "            step_size = trial.suggest_int('step_size', 15, 20)\n",
    "            #gamma_step = trial.suggest_float('gamma_step', 0.5, 0.99)\n",
    "            gamma_step = trial.suggest_float('gamma_step', 0.79, 0.93)\n",
    "            scheduler_obj = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma_step)\n",
    "        elif lr_scheduler == 'ExponentialLR':\n",
    "            #gamma_exp = trial.suggest_float('gamma_exp', 0.1, 0.99)\n",
    "            gamma_exp = trial.suggest_float('gamma_exp', 0.9872745703157462, 0.9872745703157462)\n",
    "            scheduler_obj = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma_exp)    \n",
    "            \n",
    "        train_loader = GraphDataLoader(\n",
    "            list(zip(self.subset_train_graphs, self.subset_train_labels)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate,         # custom function for combining graphs into batches\n",
    "            num_workers=0)                            # debugging TODO TODO TODO\n",
    "            #num_workers=8)  # debugging TODO TODO TODO\n",
    "        val_loader = GraphDataLoader(\n",
    "            list(zip(self.subset_val_graphs, self.subset_val_labels)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,              # we do not want to schuffle the data - we want them fixed for consistency\n",
    "            collate_fn=collate,\n",
    "            num_workers=0)\n",
    "            #num_workers=8)\n",
    "\n",
    "                          # we set the model into the training mode, but do not do the training itself --> therefore the following function\n",
    "        for epoch in range(self.num_epochs):    # training the model\n",
    "            model.train() \n",
    "            for batched_graph, labels in train_loader:\n",
    "                batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)   # moving graphs and labels into the device CPU/GPU\n",
    "                batched_graph.ndata['h'] = batched_graph.ndata['h'].to(self.device) # TODO TODO TODO - chatova práce\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())     # this line calls the model's forward pass using two inputs\n",
    "                        # batched graph ... graph structure containing connectivity information\n",
    "                        # second argument ... node features extracted from the graph, converted to floating point numbers\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()     # we calculated loss and we backpropagate the gradients\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batched_graph, labels in val_loader:\n",
    "                    batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "                    batched_graph.ndata['h'] = batched_graph.ndata['h'].to(self.device) # TODO TODO TODO - chatova práce\n",
    "                    optimizer.zero_grad()\n",
    "                    logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                    loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss = val_loss/len(val_loader)\n",
    "            trial.report(val_loss, epoch)       # we use this line to report the current validation loss at a given epoch for the ongoing trial\n",
    "                                                    # optuna is collecting these intermediates results, optuna can then stop the non-promising trials early\n",
    "            if lr_scheduler != 'None':  # using the scheduler\n",
    "                scheduler_obj.step()\n",
    "            if trial.should_prune():            # asking optuna, if trial should be pruned (\"ended\")\n",
    "                raise optuna.TrialPruned()  # TODO TODO\n",
    "        return val_loss\n",
    "\n",
    "    def optimize(self):     # running the whole hyperparameter optimization using optuna\n",
    "        sampler = optuna.samplers.TPESampler(seed=self.random_state)\n",
    "        study = optuna.create_study(direction='minimize', pruner=MedianPruner(), sampler=sampler)\n",
    "            # Medianpruner ... stops unpromising trials based on the median performance so far\n",
    "        study.optimize(self.objective, n_trials=self.num_trials, n_jobs=1)  # running optimization process, specifies, how many different sets of hyperparameters (different trials) to try\n",
    "        best_hyperparameters = study.best_trial.params\n",
    "        with open(f'sage_best_hyperparams.json', 'w') as f:    # saving the trials into a JSON file\n",
    "            json.dump(best_hyperparameters, f)\n",
    "        print(f\"Best hyperparameters are {best_hyperparameters}.\")  # printing the best hyperparameters\n",
    "        top_trials = sorted(study.trials, key=lambda t: t.value)[:10]\n",
    "\n",
    "        for i, trial in enumerate(top_trials, 1):\n",
    "            print(f\"\\nTrial #{i}\")\n",
    "            print(f\"  Value (Objective): {trial.value}\")\n",
    "            print(f\"  Params: {trial.params}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_loading(address_train, address_val, address_test, RANDOM_STATE, part=1):\n",
    "    train_graphs, train_labels_dictionary = dgl.load_graphs(address_train)\n",
    "    train_labels = train_labels_dictionary['labels']\n",
    "    train_labels = train_labels.squeeze()\n",
    "    train_labels = train_labels.numpy()\n",
    "\n",
    "    val_graphs, val_labels_dictionary = dgl.load_graphs(address_val)\n",
    "    val_labels = val_labels_dictionary['labels']\n",
    "    val_labels = val_labels.squeeze()\n",
    "    val_labels = val_labels.numpy()\n",
    "\n",
    "    test_graphs, test_labels_dictionary = dgl.load_graphs(address_test)\n",
    "    test_labels = test_labels_dictionary['labels']\n",
    "    test_labels = test_labels.squeeze()\n",
    "    test_labels = test_labels.numpy()\n",
    "\n",
    "    subset_train_indices = np.random.choice(\n",
    "        len(train_graphs), size=int(len(train_graphs) * 0.2), replace=False # we choose 20% of the training graphs\n",
    "    )   # replace=False ensures that no index is selected more than once\n",
    "\n",
    "    subset_train_graphs = [train_graphs[i] for i in subset_train_indices] # we store the corresponding graphs\n",
    "    subset_train_labels = train_labels[subset_train_indices]    # we store the corresponding labels\n",
    "\n",
    "    subset_val_indices = np.random.choice(\n",
    "        len(val_graphs), size=int(len(val_graphs) * 0.2), replace=False\n",
    "    )\n",
    "    subset_val_graphs = [train_graphs[i] for i in subset_val_indices]\n",
    "    subset_val_labels = val_labels[subset_val_indices]\n",
    "\n",
    "    combined_train_graphs = train_graphs + val_graphs # + ... list concatenation, two lists are merged into one\n",
    "    combined_train_labels = np.concatenate((train_labels, val_labels))  # two arrays merged into one\n",
    "\n",
    "    graphs = combined_train_graphs + test_graphs\n",
    "    labels_numpy = np.concatenate((combined_train_labels, test_labels))\n",
    "\n",
    "    print(\n",
    "        f'Train: {len(train_graphs)},'\n",
    "        f'Val: {len(val_graphs)},'\n",
    "        f'Test: {len(test_graphs)},'\n",
    "        f'Val + Train combined: {len(combined_train_graphs)}'\n",
    "    )\n",
    "    print(\"\\nData loading completed\\n\")\n",
    "    sys.stdout.flush()\n",
    "    return {\n",
    "        \"graphs\": (graphs, labels_numpy),\n",
    "        \"train\": (train_graphs, train_labels),\n",
    "        \"val\": (val_graphs, val_labels),\n",
    "        \"test\": (test_graphs, test_labels),\n",
    "        \"subset_train\": (subset_train_graphs, subset_train_labels),\n",
    "        \"subset_val\": (subset_val_graphs, subset_val_labels),\n",
    "        \"combined_train\": (combined_train_graphs, combined_train_labels),\n",
    "    }\n",
    "    \n",
    "def main_train_loop(run_number):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # TODO ?? what is cuda??\n",
    "    RANDOM_STATE = 42\n",
    "    # setting the random state:\n",
    "    random.seed(RANDOM_STATE)\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "    torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_STATE)\n",
    "\n",
    "    \n",
    "    num_trials = 160\n",
    "    num_epochs = 100\n",
    "    tune_hyperparams = True\n",
    "    split_num = 1 #run_number\n",
    "    \n",
    "    data = data_loading(f\"hERG_graphs/herg_graphs_train{split_num}.bin\", \n",
    "                        f\"hERG_graphs/herg_graphs_val{split_num}.bin\",\n",
    "                        f\"hERG_graphs/herg_graphs_test{split_num}.bin\", RANDOM_STATE, part=1)\n",
    "    graphs,_ = data[\"graphs\"]\n",
    "    train_graphs, train_labels = data[\"train\"]\n",
    "    val_graphs, val_labels = data[\"val\"]\n",
    "    test_graphs, test_labels = data[\"test\"]\n",
    "    subset_train_graphs, subset_train_labels = data[\"subset_train\"]\n",
    "    subset_val_graphs, subset_val_labels = data[\"subset_val\"]\n",
    "    combined_train_graphs, combined_train_labels = data[\"combined_train\"]\n",
    "    \n",
    "\n",
    "    print('Hyperparameter optimization...\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if tune_hyperparams:\n",
    "        optimizer = Hyperparameter_optimizer(\n",
    "            device,\n",
    "            subset_train_graphs = train_graphs,\n",
    "            subset_train_labels = train_labels,\n",
    "            subset_val_graphs = val_graphs,\n",
    "            subset_val_labels = val_labels,\n",
    "            #subset_train_graphs = subset_train_graphs,\n",
    "            #subset_train_labels = subset_train_labels,\n",
    "            #subset_val_graphs = subset_val_graphs,\n",
    "            #subset_val_labels = subset_val_labels,\n",
    "            #num_trials=200,\n",
    "            num_trials=num_trials,\n",
    "            #num_epochs=100)   # TODO TODO TODO DEBUGGING\n",
    "            num_epochs=num_epochs,\n",
    "            random_state=RANDOM_STATE)\n",
    "\n",
    "        optimizer.optimize()\n",
    "    \n",
    "        print(\"Hyperparameter optimization done.\")\n",
    "        sys.stdout.flush()\n",
    "        print(\"\")\n",
    "\n",
    "#def retrain_using_best_parameters(train_graphs, val_graphs, test_graphs):\n",
    "    # we load the data using GraphDataLoader\n",
    "    with open(f'sage_best_hyperparams.json', 'r') as f:\n",
    "        best_hyperparameters = json.load(f)     # we load the best hyperparameters\n",
    "    print(best_hyperparameters)\n",
    "    train_loader = GraphDataLoader(\n",
    "        list(zip(train_graphs, train_labels)),\n",
    "        batch_size=best_hyperparameters['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate,\n",
    "        num_workers=0)\n",
    "        #num_workers=8)\n",
    "    val_loader = GraphDataLoader(\n",
    "        list(zip(val_graphs, val_labels)),\n",
    "        batch_size=best_hyperparameters['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate,\n",
    "        num_workers=0)\n",
    "        #num_workers=8)\n",
    "    test_loader = GraphDataLoader(\n",
    "        list(zip(test_graphs, test_labels)),\n",
    "        batch_size=best_hyperparameters['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate,\n",
    "        num_workers=0)\n",
    "        #num_workers=8)\n",
    "    combined_test_val_loader = GraphDataLoader(\n",
    "        list(\n",
    "            zip(\n",
    "                combined_train_graphs, combined_train_labels\n",
    "            )),\n",
    "        batch_size=best_hyperparameters['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate,\n",
    "        num_workers=0)\n",
    "        #num_workers=8)\n",
    "    print(\"Dataloaders done.\")\n",
    "    print(\"Retraining using best parameters...\")\n",
    "    print(f\"Number of available node features (in_feats): {graphs[0].ndata['h'].shape[1]}\")  # Check available node features\n",
    "    model = SAGE(              # initializing model with sampled hyperparameters\n",
    "        in_feats=74,    # adjusting according to the dataset TODO TODO TODO !!!\n",
    "        out_feats=best_hyperparameters['hidden_dim'],\n",
    "        dropout_rate=best_hyperparameters['dropout_rate'],\n",
    "        aggregator_type=best_hyperparameters['aggregator_type'],\n",
    "        n_cls=2,\n",
    "        n_hidden_layers=best_hyperparameters['n_hidden_layers'],\n",
    "        activation = best_hyperparameters['activation']\n",
    "    ).to(device)\n",
    "    model.reset_parameters()    # reseting the parameters of the model before retraining\n",
    "    \n",
    "    class_counts = np.bincount(combined_train_labels)\n",
    "    class_counts[class_counts == 0] = 1\n",
    "    class_weights = 1.0 / class_counts\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparameters['lr'])\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if best_hyperparameters['lr_scheduler'] == 'None':\n",
    "        lr_scheduler = None\n",
    "    elif best_hyperparameters['lr_scheduler'] == 'StepLR':\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, best_hyperparameters['step_size'], best_hyperparameters['gamma_step'])\n",
    "    elif best_hyperparameters['lr_scheduler'] == 'ExponentialLR':\n",
    "         lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, best_hyperparameters['gamma_exp'])\n",
    "\n",
    "    #early_stopping = EarlyStopping(patience_loss=10, patience_accuracy=10, verbose=False, delta_loss=0.001, delta_accuracy=0.001) - PUVODNI SETTING!\n",
    "    early_stopping = EarlyStopping(patience_loss=80, patience_mcc=80, verbose=False, delta_loss=0.001, delta_mcc=0.001)\n",
    "    training = Training(device)\n",
    "    training.train_and_evaluate(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        early_stopping,\n",
    "        1000,\n",
    "        #1000,                # Number of epochs TODO TODO TODO ... this one WAS 300, for debugging purposes 5\n",
    "        scheduler=lr_scheduler\n",
    "    )\n",
    "    optimal_epoch = early_stopping.best_epoch\n",
    "    print(f\"The best epoch was {optimal_epoch}\") \n",
    "    model.reset_parameters()    # before we train the model on test + val dataset, we reset all the parameters\n",
    "    print(\"Training done.\")\n",
    "    print(\"Final training...\")\n",
    "    \n",
    "    class_counts = np.bincount(combined_train_labels)\n",
    "    class_counts[class_counts == 0] = 1\n",
    "    class_weights = 1.0 / class_counts\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparameters['lr'])\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "    \n",
    "    lr_scheduler = None\n",
    "    if best_hyperparameters['lr_scheduler'] == 'None':\n",
    "        lr_scheduler = None\n",
    "    elif best_hyperparameters['lr_scheduler'] == 'StepLR':\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, best_hyperparameters['step_size'], best_hyperparameters['gamma_step'])\n",
    "    elif best_hyperparameters['lr_scheduler'] == 'ExponentialLR':\n",
    "         lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, best_hyperparameters['gamma_exp'])\n",
    "    \n",
    "    \n",
    "    training.train_and_evaluate(\n",
    "        model,\n",
    "        combined_test_val_loader,\n",
    "        None,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        None,\n",
    "        optimal_epoch,\n",
    "        scheduler=lr_scheduler\n",
    "    )\n",
    "    torch.save(model.state_dict(), f'sage_model.pt')\n",
    "    print(\"Training done.\")\n",
    "    print(\"Evaluating on test_dataset\")\n",
    "    training.evaluate_on_test(model, test_loader, criterion)\n",
    "    print(\"Evaluation done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This ensures that multiprocessing works safely when used with PyTorch DataLoader\n",
    "    import multiprocessing\n",
    "    multiprocessing.set_start_method('spawn', force=True)  # Ensure spawn method is used for safe process spawning\n",
    "\n",
    "\n",
    "    # odstranit!\n",
    "    run_number = 1\n",
    "    main_train_loop(run_number)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b539f43-48dc-4e53-b493-9531ad127fbb",
   "metadata": {
    "papermill": {
     "duration": 0.022753,
     "end_time": "2025-05-31T12:51:38.937521",
     "exception": false,
     "start_time": "2025-05-31T12:51:38.914768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7772.705468,
   "end_time": "2025-05-31T12:51:41.608083",
   "environment_variables": {},
   "exception": null,
   "input_path": "GraphSAGE_model.ipynb",
   "output_path": "run_out_hERG/GraphSAGE_tune3_out1.ipynb",
   "parameters": {},
   "start_time": "2025-05-31T10:42:08.902615",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
